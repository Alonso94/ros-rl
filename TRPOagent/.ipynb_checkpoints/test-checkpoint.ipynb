{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_crumb\n",
    "from agent.agent_TRPO import TRPOAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airan/PycharmProjects/TRPOagent/network/network.py:59: UserWarning: The parameter 'updates' of theano.function() expects an OrderedDict, got <class 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (theano.compat.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n",
      "  self.load_flat_weights = theano.function([flat_weights_placeholder], updates=dict(zip(self.weights, assigns)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Iteration 1 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 35\n",
      "Average sum of rewards per episode:       84.23257142857142\n",
      "Std of rewards per episode:               120.74244845580215\n",
      "Entropy:                                  -17864.73290596111\n",
      "KL between old and new distribution:      0.00998846668837899\n",
      "Surrogate loss:                           -33735.00262591315\n",
      "\n",
      "********** Iteration 2 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 50\n",
      "Average sum of rewards per episode:       129.91660000000002\n",
      "Std of rewards per episode:               173.61691714357795\n",
      "Entropy:                                  -20036.42032291792\n",
      "KL between old and new distribution:      0.009971126796025034\n",
      "Surrogate loss:                           -63626.099347295094\n",
      "\n",
      "********** Iteration 3 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 60\n",
      "Average sum of rewards per episode:       127.55933333333336\n",
      "Std of rewards per episode:               162.64347304115492\n",
      "Entropy:                                  -22476.06812276672\n",
      "KL between old and new distribution:      0.009990379473392935\n",
      "Surrogate loss:                           -79456.74992195211\n",
      "\n",
      "********** Iteration 4 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 60\n",
      "Average sum of rewards per episode:       147.08599999999998\n",
      "Std of rewards per episode:               135.79832112609736\n",
      "Entropy:                                  -22327.996787831762\n",
      "KL between old and new distribution:      0.009980713976785138\n",
      "Surrogate loss:                           -84661.59583247863\n",
      "\n",
      "********** Iteration 5 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 63\n",
      "Average sum of rewards per episode:       141.97619047619048\n",
      "Std of rewards per episode:               125.24490520865758\n",
      "Entropy:                                  -20670.40038818721\n",
      "KL between old and new distribution:      0.009966994565480516\n",
      "Surrogate loss:                           -95647.47904898164\n",
      "\n",
      "********** Iteration 6 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 75\n",
      "Average sum of rewards per episode:       191.33986666666667\n",
      "Std of rewards per episode:               146.91124383103636\n",
      "Entropy:                                  -22760.587856778067\n",
      "KL between old and new distribution:      0.009956893641250044\n",
      "Surrogate loss:                           -123525.82821585122\n",
      "\n",
      "********** Iteration 7 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 65\n",
      "Average sum of rewards per episode:       185.94169230769228\n",
      "Std of rewards per episode:               126.18082818618477\n",
      "Entropy:                                  -20502.60933587303\n",
      "KL between old and new distribution:      0.009976563255721393\n",
      "Surrogate loss:                           -110328.02808380361\n",
      "\n",
      "********** Iteration 8 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 75\n",
      "Average sum of rewards per episode:       221.59573333333333\n",
      "Std of rewards per episode:               184.28017470633011\n",
      "Entropy:                                  -22719.95119446068\n",
      "KL between old and new distribution:      0.009970461683556291\n",
      "Surrogate loss:                           -149636.2124189496\n",
      "\n",
      "********** Iteration 9 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 66\n",
      "Average sum of rewards per episode:       224.71924242424242\n",
      "Std of rewards per episode:               178.2287170201205\n",
      "Entropy:                                  -22614.09466897711\n",
      "KL between old and new distribution:      0.009960056653410404\n",
      "Surrogate loss:                           -131202.2928441016\n",
      "\n",
      "********** Iteration 10 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 81\n",
      "Average sum of rewards per episode:       223.3322222222222\n",
      "Std of rewards per episode:               178.57928569409262\n",
      "Entropy:                                  -22570.90584576161\n",
      "KL between old and new distribution:      0.009964993051974437\n",
      "Surrogate loss:                           -172008.1459700286\n",
      "\n",
      "********** Iteration 11 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 79\n",
      "Average sum of rewards per episode:       214.57481012658232\n",
      "Std of rewards per episode:               153.7089098894216\n",
      "Entropy:                                  -22334.00389617905\n",
      "KL between old and new distribution:      0.009937286320529459\n",
      "Surrogate loss:                           -154256.34357510143\n",
      "\n",
      "********** Iteration 12 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 97\n",
      "Average sum of rewards per episode:       216.3607216494845\n",
      "Std of rewards per episode:               151.1073494992057\n",
      "Entropy:                                  -19944.16541725691\n",
      "KL between old and new distribution:      0.009903593628829976\n",
      "Surrogate loss:                           -182505.0630795206\n",
      "\n",
      "********** Iteration 13 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 74\n",
      "Average sum of rewards per episode:       244.0972972972973\n",
      "Std of rewards per episode:               185.18225215802138\n",
      "Entropy:                                  -22343.911946224343\n",
      "KL between old and new distribution:      0.009972196738248689\n",
      "Surrogate loss:                           -175746.5696909766\n",
      "\n",
      "********** Iteration 14 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 96\n",
      "Average sum of rewards per episode:       242.25052083333333\n",
      "Std of rewards per episode:               165.57952016056737\n",
      "Entropy:                                  -22121.933833323183\n",
      "KL between old and new distribution:      0.009889837036543185\n",
      "Surrogate loss:                           -204275.17717850895\n",
      "\n",
      "********** Iteration 15 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 101\n",
      "Average sum of rewards per episode:       228.86346534653467\n",
      "Std of rewards per episode:               225.5539741532128\n",
      "Entropy:                                  -21978.335699404077\n",
      "KL between old and new distribution:      0.009899646527650475\n",
      "Surrogate loss:                           -200768.41252290417\n",
      "\n",
      "********** Iteration 16 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 114\n",
      "Average sum of rewards per episode:       237.94096491228072\n",
      "Std of rewards per episode:               169.89596416718385\n",
      "Entropy:                                  -21838.52229804228\n",
      "KL between old and new distribution:      0.00994227980654402\n",
      "Surrogate loss:                           -231505.63770564157\n",
      "\n",
      "********** Iteration 17 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 118\n",
      "Average sum of rewards per episode:       213.94415254237285\n",
      "Std of rewards per episode:               174.4015278628372\n",
      "Entropy:                                  -21773.080405557102\n",
      "KL between old and new distribution:      0.009931697759273198\n",
      "Surrogate loss:                           -216145.21494651982\n",
      "\n",
      "********** Iteration 18 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 109\n",
      "Average sum of rewards per episode:       228.30256880733944\n",
      "Std of rewards per episode:               173.6243033890883\n",
      "Entropy:                                  -21848.26670687901\n",
      "KL between old and new distribution:      0.009964625456808594\n",
      "Surrogate loss:                           -216596.74425995455\n",
      "\n",
      "********** Iteration 19 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 131\n",
      "Average sum of rewards per episode:       217.7832824427481\n",
      "Std of rewards per episode:               191.04682226717586\n",
      "Entropy:                                  -21584.414151091147\n",
      "KL between old and new distribution:      0.009961580837231112\n",
      "Surrogate loss:                           -242264.1551286856\n",
      "\n",
      "********** Iteration 20 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 129\n",
      "Average sum of rewards per episode:       224.37883720930233\n",
      "Std of rewards per episode:               140.67553592833914\n",
      "Entropy:                                  -21380.2317101311\n",
      "KL between old and new distribution:      0.009890380594121702\n",
      "Surrogate loss:                           -236703.54851532652\n",
      "\n",
      "********** Iteration 21 ************\n",
      "Rollout\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"crumb-synthetic-v0\")\n",
    "agent = TRPOAgent(env)\n",
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
