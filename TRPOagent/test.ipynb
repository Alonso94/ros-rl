{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_crumb\n",
    "from agent.agent_TRPO import TRPOAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "if os.environ.get(\"DISPLAY\") is str and len(os.environ.get(\"DISPLAY\"))!=0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"crumb-synthetic-v0\")\n",
    "agent = TRPOAgent(env)\n",
    "#agent.net.Loadmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Iteration 1 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1255\n",
      "Average sum of rewards per episode:       94.02505258964143\n",
      "Std of rewards per episode:               125.24830336309664\n",
      "Entropy:                                  -11119.668806582855\n",
      "KL between old and new distribution:      0.009931345692464257\n",
      "Surrogate loss:                           -283685.1429624296\n",
      "\n",
      "********** Iteration 2 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1240\n",
      "Average sum of rewards per episode:       93.56098629032259\n",
      "Std of rewards per episode:               123.58861643756771\n",
      "Entropy:                                  -11175.854088308686\n",
      "KL between old and new distribution:      0.00985592034791564\n",
      "Surrogate loss:                           -273549.88709997776\n",
      "\n",
      "********** Iteration 3 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1258\n",
      "Average sum of rewards per episode:       99.71114864864865\n",
      "Std of rewards per episode:               122.05357989697433\n",
      "Entropy:                                  -10547.424895068758\n",
      "KL between old and new distribution:      0.009842947047153038\n",
      "Surrogate loss:                           -295443.9860533706\n",
      "\n",
      "********** Iteration 4 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1296\n",
      "Average sum of rewards per episode:       100.9730925925926\n",
      "Std of rewards per episode:               124.35923478299637\n",
      "Entropy:                                  -10475.901106529118\n",
      "KL between old and new distribution:      0.009987966063774249\n",
      "Surrogate loss:                           -290111.12960342196\n",
      "\n",
      "********** Iteration 5 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1287\n",
      "Average sum of rewards per episode:       106.79910878010877\n",
      "Std of rewards per episode:               125.60440546635049\n",
      "Entropy:                                  -10681.808634493837\n",
      "KL between old and new distribution:      0.009933463724151988\n",
      "Surrogate loss:                           -314996.7204310529\n",
      "\n",
      "********** Iteration 6 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1281\n",
      "Average sum of rewards per episode:       105.26009758001561\n",
      "Std of rewards per episode:               125.79301854766805\n",
      "Entropy:                                  -10631.280240404585\n",
      "KL between old and new distribution:      0.009851760271572396\n",
      "Surrogate loss:                           -312381.3953476802\n",
      "\n",
      "********** Iteration 7 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1278\n",
      "Average sum of rewards per episode:       105.69939280125197\n",
      "Std of rewards per episode:               126.68494918565194\n",
      "Entropy:                                  -10062.16390574291\n",
      "KL between old and new distribution:      0.009903996105944776\n",
      "Surrogate loss:                           -323224.8358839393\n",
      "\n",
      "********** Iteration 8 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1277\n",
      "Average sum of rewards per episode:       103.81456147220048\n",
      "Std of rewards per episode:               125.64977360088378\n",
      "Entropy:                                  -9989.772145124336\n",
      "KL between old and new distribution:      0.009980420576675579\n",
      "Surrogate loss:                           -307980.84662440943\n",
      "\n",
      "********** Iteration 9 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1318\n",
      "Average sum of rewards per episode:       113.51343778452201\n",
      "Std of rewards per episode:               131.6141889264936\n",
      "Entropy:                                  -9968.121088095559\n",
      "KL between old and new distribution:      0.009970330586982174\n",
      "Surrogate loss:                           -342222.6577157058\n",
      "\n",
      "********** Iteration 10 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1318\n",
      "Average sum of rewards per episode:       116.29162746585735\n",
      "Std of rewards per episode:               128.69646737601354\n",
      "Entropy:                                  -9547.638147753587\n",
      "KL between old and new distribution:      0.009798977108083223\n",
      "Surrogate loss:                           -349129.1261509071\n",
      "\n",
      "********** Iteration 11 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1261\n",
      "Average sum of rewards per episode:       109.00909436954797\n",
      "Std of rewards per episode:               127.3390367090435\n",
      "Entropy:                                  -9796.442669540287\n",
      "KL between old and new distribution:      0.009822629016689313\n",
      "Surrogate loss:                           -325109.3533445562\n",
      "\n",
      "********** Iteration 12 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1317\n",
      "Average sum of rewards per episode:       121.10402353834473\n",
      "Std of rewards per episode:               130.2519245503762\n",
      "Entropy:                                  -9188.638916260275\n",
      "KL between old and new distribution:      0.009885137303993695\n",
      "Surrogate loss:                           -370007.9601963096\n",
      "\n",
      "********** Iteration 13 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1318\n",
      "Average sum of rewards per episode:       117.44548558421853\n",
      "Std of rewards per episode:               132.12604009489283\n",
      "Entropy:                                  -9284.592630331254\n",
      "KL between old and new distribution:      0.00983108471726253\n",
      "Surrogate loss:                           -359258.25447010034\n",
      "\n",
      "********** Iteration 14 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1358\n",
      "Average sum of rewards per episode:       127.92928571428571\n",
      "Std of rewards per episode:               133.71986577801957\n",
      "Entropy:                                  -8944.551898120255\n",
      "KL between old and new distribution:      0.009888049181859336\n",
      "Surrogate loss:                           -394938.4992543258\n",
      "\n",
      "********** Iteration 15 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1335\n",
      "Average sum of rewards per episode:       123.52807940074909\n",
      "Std of rewards per episode:               131.44571233883917\n",
      "Entropy:                                  -9039.816478185161\n",
      "KL between old and new distribution:      0.009956270923264843\n",
      "Surrogate loss:                           -373191.9710126028\n",
      "\n",
      "********** Iteration 16 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1352\n",
      "Average sum of rewards per episode:       119.15228254437869\n",
      "Std of rewards per episode:               126.2636954572308\n",
      "Entropy:                                  -8851.10444166488\n",
      "KL between old and new distribution:      0.009907365880234983\n",
      "Surrogate loss:                           -365309.136478823\n",
      "\n",
      "********** Iteration 17 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1359\n",
      "Average sum of rewards per episode:       126.49179985283297\n",
      "Std of rewards per episode:               129.21893640399657\n",
      "Entropy:                                  -8618.194450028705\n",
      "KL between old and new distribution:      0.009879472529546575\n",
      "Surrogate loss:                           -378632.9268021586\n",
      "\n",
      "********** Iteration 18 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1386\n",
      "Average sum of rewards per episode:       125.49355844155843\n",
      "Std of rewards per episode:               132.91252328373417\n",
      "Entropy:                                  -8770.531215257884\n",
      "KL between old and new distribution:      0.009813860530200112\n",
      "Surrogate loss:                           -388697.0650309847\n",
      "\n",
      "********** Iteration 19 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1401\n",
      "Average sum of rewards per episode:       132.86594789436117\n",
      "Std of rewards per episode:               136.11865782304383\n",
      "Entropy:                                  -8538.814814942105\n",
      "KL between old and new distribution:      0.009820152509678427\n",
      "Surrogate loss:                           -416422.3546391721\n",
      "\n",
      "********** Iteration 20 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1453\n",
      "Average sum of rewards per episode:       139.3537666896077\n",
      "Std of rewards per episode:               135.8960409584736\n",
      "Entropy:                                  -8082.73778249445\n",
      "KL between old and new distribution:      0.009753356783125586\n",
      "Surrogate loss:                           -446726.56836563296\n",
      "\n",
      "********** Iteration 21 ************\n",
      "Rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made rollout\n",
      "Total number of episodes:                 1440\n",
      "Average sum of rewards per episode:       140.39744166666665\n",
      "Std of rewards per episode:               132.65424385051597\n",
      "Entropy:                                  -7928.074477594016\n",
      "KL between old and new distribution:      0.009936475868854478\n",
      "Surrogate loss:                           -445866.37688058603\n",
      "\n",
      "********** Iteration 22 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1397\n",
      "Average sum of rewards per episode:       139.20014674302075\n",
      "Std of rewards per episode:               134.16478563359857\n",
      "Entropy:                                  -7786.770968221724\n",
      "KL between old and new distribution:      0.00988571963766268\n",
      "Surrogate loss:                           -445204.9876274656\n",
      "\n",
      "********** Iteration 23 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1428\n",
      "Average sum of rewards per episode:       144.820100140056\n",
      "Std of rewards per episode:               140.58724574398144\n",
      "Entropy:                                  -7840.283325585986\n",
      "KL between old and new distribution:      0.009965640531514271\n",
      "Surrogate loss:                           -463273.2445741012\n",
      "\n",
      "********** Iteration 24 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1419\n",
      "Average sum of rewards per episode:       145.9258365045807\n",
      "Std of rewards per episode:               141.3454557622413\n",
      "Entropy:                                  -7623.866024978\n",
      "KL between old and new distribution:      0.009998207514879674\n",
      "Surrogate loss:                           -464995.19017416314\n",
      "\n",
      "********** Iteration 25 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1465\n",
      "Average sum of rewards per episode:       145.29876177474404\n",
      "Std of rewards per episode:               137.72911343405562\n",
      "Entropy:                                  -7009.7261323390585\n",
      "KL between old and new distribution:      0.009921944990832807\n",
      "Surrogate loss:                           -468248.4113329323\n",
      "\n",
      "********** Iteration 26 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1422\n",
      "Average sum of rewards per episode:       148.03325668073137\n",
      "Std of rewards per episode:               140.42484666714714\n",
      "Entropy:                                  -7085.37579693895\n",
      "KL between old and new distribution:      0.009903597536773976\n",
      "Surrogate loss:                           -478364.3106129494\n",
      "\n",
      "********** Iteration 27 ************\n",
      "Rollout\n",
      "Made rollout\n"
     ]
    }
   ],
   "source": [
    "rewards, i = agent.learn(reward=150, max_pathlength=5, n_timesteps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.net.Savemodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) 100.008 (0.043555775121174554, 0.001407552054996014, 0.04093742568242831, 0.014807144293025029, -0.2912763527358712, 1.4005109685912707, 0.7003307579702631, 1.3456062475206525) [-0.01299617  0.04960444 -0.06707758  0.27983246]\n"
     ]
    }
   ],
   "source": [
    "agent.play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env1 = gym.make(\"crumb-pick-v0\")\n",
    "#agent.grasp(env1)\n",
    "#agent.pick(env1)\n",
    "#agent.putdown(env1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4392827844092338,\n",
       " 0.2695451879616264,\n",
       " -0.34615669,\n",
       " 0.022142770678141632,\n",
       " 0.2695451879616264,\n",
       " -0.34615669,\n",
       " 0.022142770678141632,\n",
       " 0.2695451879616264,\n",
       " -0.34615669,\n",
       " 0.022142770678141632,\n",
       " -1.5707963267948966,\n",
       " 1.5707963267948966,\n",
       " 0.3795212282445104,\n",
       " -1.5707963267948966)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.4392827844092338, 0.2695451879616264, -0.34615669, 0.022142770678141632, 0.2695451879616264, -0.34615669, 0.022142770678141632, 0.2695451879616264, -0.34615669, 0.022142770678141632, -1.5707963267948966, 1.5707963267948966, 0.3795212282445104, -1.5707963267948966)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) 0.007000000000000006 (0.23, 0.2, 0.05, 0.07, 0.12, 0.78, 0.32, 0.39) [0.12797938 0.75730049 0.33070355 0.41555789]\n",
      "2 ) 0.017999999999999988 (0.14, 0.11, 0.02, -0.03, 0.36, 1.2, 0.47000000000000003, 0.63) [0.23687681 0.40317045 0.1144163  0.20252017]\n",
      "3 ) 0.011999999999999983 (0.15, 0.12, 0.0, -0.03, 0.35000000000000003, 1.34, 0.53, 0.88) [-0.01286168  0.17568968 -0.00823797  0.0606187 ]\n",
      "4 ) 0.008000000000000007 (0.17, 0.14, -0.05, -0.03, 0.37, 1.46, 0.64, 1.04) [ 0.02067291  0.19924721 -0.09267958 -0.03955087]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b358e1c95a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrasp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ISA/ros-rl/TRPOagent/agent/agent_TRPO.py\u001b[0m in \u001b[0;36mgrasp\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynthetic_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym-crumb/Gym-crumb/gym_crumb/envs/crumb_pick_env.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mrospy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mgripper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gripper_1_link'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mgripper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgripper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gripper_2_link'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ros/kinetic/lib/python2.7/dist-packages/rospy/timer.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(duration)\u001b[0m\n\u001b[1;32m    157\u001b[0m                   \u001b[0;32mnot\u001b[0m \u001b[0mrospy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_shutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mrostime_cond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mrostime_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrospy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrostime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rostime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0minitial_rostime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.grasp(env1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env1.arm[3].publish(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05530484, -0.29997266,  0.11164202])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def metric(a1, a2):\n",
    "    return ((a1-a2)**2).sum()**(1/2)\n",
    "\n",
    "def get_pose(joint):\n",
    "    a = env1.link_state(joint, '').link_state.pose.position\n",
    "    return np.array([a.x, a.y, a.z])\n",
    "\n",
    "def length():\n",
    "    length = np.zeros(4)\n",
    "    joint = np.zeros((4,3))\n",
    "    joint[0] = get_pose('biceps_link')\n",
    "    joint[1] = get_pose('forearm_link')\n",
    "    joint[2] = get_pose('wrist_1_link')\n",
    "    joint[3] = get_pose('gripper_1_link')/2 + get_pose('gripper_2_link')/2\n",
    "    for i in range(3):\n",
    "        length[i] = metric(joint[i+1], joint[i])\n",
    "    return length\n",
    "\n",
    "box, _ = env1.get_state()\n",
    "box = np.array([box.x, box.y, box.z])\n",
    "bic = get_pose('biceps_link')\n",
    "bic - box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aim1 = np.array([env1.aim.x, env1.aim.y, env1.aim.z])\n",
    "joint = np.zeros((4, 3))\n",
    "joint[0] = get_pose('biceps_link')\n",
    "joint[1] = get_pose('forearm_link')\n",
    "joint[2] = get_pose('wrist_1_link')\n",
    "joint[3] = get_pose('gripper_1_link')/2 + get_pose('gripper_2_link')/2\n",
    "\n",
    "# vectors\n",
    "vec1 = np.zeros((3, 3))\n",
    "vec2 = np.zeros((3, 3))\n",
    "\n",
    "for i in range(3):\n",
    "    vec1[i] = joint[3] - joint[i]\n",
    "    vec2[i] = aim1 - joint[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15000095, 0.14202985, 0.11453303, 0.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04790541,  0.43824102, -0.11164279],\n",
       "       [ 0.04456551,  0.3906848 , -0.25386632],\n",
       "       [ 0.03490639,  0.24898522, -0.25339561]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03635168, -0.13413006,  0.25199619],\n",
       "       [-0.03635168, -0.13413006,  0.25199619],\n",
       "       [-0.03635168, -0.13413006,  0.25199619]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1 - vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
